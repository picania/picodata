2024-02-08 11:55:11.198 [122699] main/103/interactive I> Tarantool 2.11.2-94-gcbd79b118 Linux-x86_64-RelWithDebInfo
2024-02-08 11:55:11.198 [122699] main/103/interactive I> log level 5
2024-02-08 11:55:11.198 [122699] main/103/interactive I> wal/engine cleanup is paused
2024-02-08 11:55:11.198 [122699] main/103/interactive I> mapping 67108864 bytes for memtx tuple arena...
2024-02-08 11:55:11.198 [122699] main/103/interactive I> Actual slab_alloc_factor calculated on the basis of desired slab_alloc_factor = 1.044274
2024-02-08 11:55:11.198 [122699] main/103/interactive I> mapping 134217728 bytes for vinyl tuple arena...
2024-02-08 11:55:11.199 [122699] main/103/interactive/box.upgrade I> Recovering snapshot with schema version 2.11.1
2024-02-08 11:55:11.200 [122699] main/103/interactive I> update replication_synchro_quorum = 1
2024-02-08 11:55:11.200 [122699] main/103/interactive I> instance uuid 68d4a766-4144-3248-aeb4-e212356716e4
2024-02-08 11:55:11.200 [122699] main/103/interactive I> instance vclock {0: 489, 1: 3137}
2024-02-08 11:55:11.200 [122699] main/103/interactive I> recovery start
2024-02-08 11:55:11.200 [122699] main/103/interactive I> recovering from `./00000000000000000000.snap'
2024-02-08 11:55:11.200 [122699] main/103/interactive I> cluster uuid e0df68c5-e7f9-395f-86b3-30ad9e1b7b07
2024-02-08 11:55:11.208 [122699] main/103/interactive I> assigned id 1 to replica 68d4a766-4144-3248-aeb4-e212356716e4
2024-02-08 11:55:11.208 [122699] main/103/interactive I> update replication_synchro_quorum = 1
2024-02-08 11:55:11.208 [122699] main/103/interactive I> recover from `./00000000000000000000.xlog'
2024-02-08 11:55:11.212 [122699] main/103/interactive I> done `./00000000000000000000.xlog'
2024-02-08 11:55:11.212 [122699] main/103/interactive I> recover from `./00000000000000003325.xlog'
2024-02-08 11:55:11.212 [122699] main/103/interactive I> done `./00000000000000003325.xlog'
2024-02-08 11:55:11.212 [122699] main/103/interactive I> recover from `./00000000000000003368.xlog'
2024-02-08 11:55:11.212 [122699] main/103/interactive I> done `./00000000000000003368.xlog'
2024-02-08 11:55:11.213 [122699] main/103/interactive I> recover from `./00000000000000003411.xlog'
2024-02-08 11:55:11.213 [122699] main/103/interactive I> done `./00000000000000003411.xlog'
2024-02-08 11:55:11.213 [122699] main/103/interactive I> recover from `./00000000000000003454.xlog'
2024-02-08 11:55:11.213 [122699] main/103/interactive I> done `./00000000000000003454.xlog'
2024-02-08 11:55:11.213 [122699] main/103/interactive I> recover from `./00000000000000003497.xlog'
2024-02-08 11:55:11.213 [122699] main/103/interactive I> done `./00000000000000003497.xlog'
2024-02-08 11:55:11.213 [122699] main/103/interactive I> recover from `./00000000000000003540.xlog'
2024-02-08 11:55:11.213 [122699] main/103/interactive I> done `./00000000000000003540.xlog'
2024-02-08 11:55:11.213 [122699] main/103/interactive I> recover from `./00000000000000003583.xlog'
2024-02-08 11:55:11.213 [122699] main/103/interactive I> done `./00000000000000003583.xlog'
2024-02-08 11:55:11.213 [122699] main/103/interactive I> recover from `./00000000000000003626.xlog'
2024-02-08 11:55:11.213 [122699] main/103/interactive I> done `./00000000000000003626.xlog'
2024-02-08 11:55:11.214 [122699] main/103/interactive I> Building secondary indexes in space '_pico_table'...
2024-02-08 11:55:11.214 [122699] main/103/interactive I> Adding 11 keys to TREE index 'name' ...
2024-02-08 11:55:11.214 [122699] main/103/interactive I> Space '_pico_table': done
2024-02-08 11:55:11.214 [122699] main/103/interactive I> Building secondary indexes in space '_pico_instance'...
2024-02-08 11:55:11.214 [122699] main/103/interactive I> Adding 1 keys to TREE index 'raft_id' ...
2024-02-08 11:55:11.214 [122699] main/103/interactive I> Adding 1 keys to TREE index 'replicaset_id' ...
2024-02-08 11:55:11.214 [122699] main/103/interactive I> Space '_pico_instance': done
2024-02-08 11:55:11.214 [122699] main/103/interactive I> Building secondary indexes in space '_pico_user'...
2024-02-08 11:55:11.214 [122699] main/103/interactive I> Adding 3 keys to TREE index 'name' ...
2024-02-08 11:55:11.214 [122699] main/103/interactive I> Space '_pico_user': done
2024-02-08 11:55:11.214 [122699] main/103/interactive I> Building secondary indexes in space '_pico_privilege'...
2024-02-08 11:55:11.214 [122699] main/103/interactive I> Adding 11 keys to TREE index 'object' ...
2024-02-08 11:55:11.214 [122699] main/103/interactive I> Space '_pico_privilege': done
2024-02-08 11:55:11.214 [122699] main/103/interactive I> Building secondary indexes in space '_bucket'...
2024-02-08 11:55:11.214 [122699] main/103/interactive I> Adding 3000 keys to TREE index 'status' ...
2024-02-08 11:55:11.217 [122699] main/103/interactive I> Space '_bucket': done
2024-02-08 11:55:11.217 [122699] main/103/interactive I> Building secondary indexes in space '_pico_role'...
2024-02-08 11:55:11.217 [122699] main/103/interactive I> Adding 2 keys to TREE index 'name' ...
2024-02-08 11:55:11.217 [122699] main/103/interactive I> Space '_pico_role': done
2024-02-08 11:55:11.217 [122699] main/103/interactive I> ready to accept requests
2024-02-08 11:55:11.217 [122699] main/103/interactive I> leaving orphan mode
2024-02-08 11:55:11.217 [122699] main/105/gc I> wal/engine cleanup is resumed
2024-02-08 11:55:11.217 [122699] main/103/interactive/box.load_cfg I> set 'memtx_memory' configuration option to 67108864
2024-02-08 11:55:11.217 [122699] main/103/interactive/box.load_cfg I> set 'log' configuration option to " "
2024-02-08 11:55:11.217 [122699] main/103/interactive/box.load_cfg I> set 'replication' configuration option to []
2024-02-08 11:55:11.217 [122699] main/103/interactive/box.load_cfg I> set 'replication_connect_quorum' configuration option to 32
2024-02-08 11:55:11.217 [122699] main/103/interactive/box.load_cfg I> set 'bootstrap_strategy' configuration option to "legacy"
2024-02-08 11:55:11.230 [122699] main/103/interactive/box.load_cfg load_cfg.lua:738 W> Deprecated option replication_connect_quorum, please use bootstrap_strategy instead
2024-02-08 11:55:11.230 [122699] main/106/checkpoint_daemon I> scheduled next checkpoint for Thu Feb  8 13:23:10 2024
2024-02-08 11:55:11.230 [122699] main/103/interactive I> tx_binary: bound to 127.0.0.1:3301
2024-02-08 11:55:11.230 [122699] main/103/interactive/box.load_cfg I> set 'listen' configuration option to "localhost:3301"
2024-02-08 11:55:11.231 [122699] main/103/interactive/box.load_cfg I> set 'read_only' configuration option to true
2024-02-08 11:55:11.231 [122699] main/103/interactive I> >>>>> postjoin()
2024-02-08 11:55:11.231 [122699] main/103/interactive/box.load_cfg load_cfg.lua:738 W> Deprecated option replication_connect_quorum, please use bootstrap_strategy instead
2024-02-08 11:55:11.231 [122699] main/103/interactive I> leaving orphan mode
2024-02-08 11:55:11.231 [122699] main/103/interactive/box.load_cfg I> set 'replication_connect_quorum' configuration option to 0
2024-02-08 11:55:11.231 [122699] main/103/interactive I> switched to configuration, config: Configuration { voters: Configuration { incoming: Configuration { voters: {1} }, outgoing: Configuration { voters: {} } }, learners: {}, learners_next: {}, auto_leave: false }, raft_id: 1
2024-02-08 11:55:11.232 [122699] main/103/interactive I> became follower at term 9, term: 9, raft_id: 1
2024-02-08 11:55:11.232 [122699] main/103/interactive I> newRaft, peers: Configuration { incoming: Configuration { voters: {1} }, outgoing: Configuration { voters: {} } }, last term: 9, last index: 122, applied: 122, commit: 122, term: 9, raft_id: 1
2024-02-08 11:55:11.232 [122699] main/103/interactive I> RawNode created with id 1., id: 1, raft_id: 1
2024-02-08 11:55:11.232 [122699] main/113/sentinel_loop I> waiting until initialized...
2024-02-08 11:55:11.232 [122699] main/103/interactive I> this is the only voter in cluster, triggering election immediately
2024-02-08 11:55:11.232 [122699] main/103/interactive I> starting a new election, term: 9, raft_id: 1
2024-02-08 11:55:11.232 [122699] main/103/interactive I> became pre-candidate at term 9, term: 9, raft_id: 1
2024-02-08 11:55:11.232 [122699] main/103/interactive I> became candidate at term 10, term: 10, raft_id: 1
2024-02-08 11:55:11.232 [122699] main/103/interactive I> became leader at term 10, term: 10, raft_id: 1
2024-02-08 11:55:11.232 [122699] main/103/interactive/box.load_cfg load_cfg.lua:738 W> Deprecated option replication_connect_quorum, please use bootstrap_strategy instead
2024-02-08 11:55:11.233 [122699] main/103/interactive/socket I> tcp_server: remove dead UNIX socket: ./admin.sock
2024-02-08 11:55:11.233 [122699] main/112/governor_loop I> target master i1 of replicaset r1 is not online: trying to choose a new one
2024-02-08 11:55:11.233 [122699] main/112/governor_loop plan.rs:545 W> there are no instances suitable as master of replicaset r1
2024-02-08 11:55:11.233 [122699] main/112/governor_loop I> nothing to do, waiting for events to handle
2024-02-08 11:55:11.233 [122699] main/114/console/unix/:./admin.sock/socket I> started
2024-02-08 11:55:11.233 [122699] main/103/interactive I> initiating self-activation of i1
2024-02-08 11:55:11.337 [122699] main/112/governor_loop I> configuring replication
2024-02-08 11:55:11.337 [122699] main/112/governor_loop I> calling rpc::replication, instance_id: i1
2024-02-08 11:55:11.338 [122699] main/103/interactive I> self-activated successfully
2024-02-08 11:55:11.338 [122699] main I> entering the event loop
2024-02-08 11:55:11.339 [122699] main/110/.proc_replication I> connecting to 1 replicas
2024-02-08 11:55:11.339 [122699] main/110/.proc_replication C> failed to connect to 1 out of 1 replicas
2024-02-08 11:55:11.339 [122699] main/110/.proc_replication I> leaving orphan mode
2024-02-08 11:55:11.339 [122699] main/110/.proc_replication/box.load_cfg I> set 'replication' configuration option to ["pico_service@localhost:3301"]
2024-02-08 11:55:11.339 [122699] main/110/.proc_replication/box.load_cfg I> set 'read_only' configuration option to false
2024-02-08 11:55:11.339 [122699] main/112/governor_loop I> configured replication with instance, lsn: 3137, instance_id: i1
2024-02-08 11:55:11.339 [122699] main/112/governor_loop I> handling instance grade change, current_grade: Replicated(9), instance_id: i1
2024-02-08 11:55:11.340 [122699] main/121/applier/pico_service@localhost:3301 I> remote master 68d4a766-4144-3248-aeb4-e212356716e4 at 127.0.0.1:3301 running Tarantool 2.11.2
2024-02-08 11:55:11.340 [122699] main/121/applier/pico_service@localhost:3301 I> leaving orphan mode
2024-02-08 11:55:11.441 [122699] main/112/governor_loop I> updating target vshard config
2024-02-08 11:55:11.441 [122699] main/112/governor_loop I> applying vshard config changes
2024-02-08 11:55:11.441 [122699] main/112/governor_loop I> calling rpc::sharding, instance_id: i1
2024-02-08 11:55:11.442 [122699] main/110/.proc_sharding/vshard.storage I> Starting configuration of replica 68d4a766-4144-3248-aeb4-e212356716e4
2024-02-08 11:55:11.442 [122699] main/110/.proc_sharding/vshard.storage I> I am master
2024-02-08 11:55:11.442 [122699] main/110/.proc_sharding/vshard.storage I> Taking on replicaset master role...
2024-02-08 11:55:11.442 [122699] main/110/.proc_sharding I> tx_binary: stopped
2024-02-08 11:55:11.443 [122699] main/110/.proc_sharding I> tx_binary: bound to 127.0.0.1:3301
2024-02-08 11:55:11.443 [122699] main/110/.proc_sharding/box.load_cfg I> set 'listen' configuration option to "127.0.0.1:3301"
2024-02-08 11:55:11.443 [122699] main/110/.proc_sharding/vshard.storage I> Box has been configured
2024-02-08 11:55:11.443 [122699] main/125/lua/vshard.util I> gc_bucket_f has been started
2024-02-08 11:55:11.443 [122699] main/125/lua/box.schema schema.lua:1832 W> box.internal.schema_version will be removed, please use box.info.schema_version instead
2024-02-08 11:55:11.443 [122699] main/126/lua/vshard.util I> recovery_f has been started
2024-02-08 11:55:11.443 [122699] main/110/.proc_sharding/vshard.storage I> Took on replicaset master role
2024-02-08 11:55:11.443 [122699] main/127/lua/vshard.util I> rebalancer_f has been started
2024-02-08 11:55:11.448 [122699] main/110/.proc_sharding/vshard.router I> Starting router configuration
2024-02-08 11:55:11.448 [122699] main/110/.proc_sharding/vshard.router I> Calling box.cfg()...
2024-02-08 11:55:11.448 [122699] main/110/.proc_sharding/vshard.router I> {"listen":"127.0.0.1:3301"}
2024-02-08 11:55:11.448 [122699] main/110/.proc_sharding/vshard.router I> Box has been configured
2024-02-08 11:55:11.449 [122699] main/128/localhost:3301 (net.box)/vshard.replicaset I> connected to localhost:3301
2024-02-08 11:55:11.449 [122699] main/129/localhost:3301 (net.box)/vshard.replicaset I> connected to localhost:3301
2024-02-08 11:55:11.449 [122699] main/130/lua/vshard.util I> failover_f has been started
2024-02-08 11:55:11.449 [122699] main/130/lua/vshard.router I> New replica i1(pico_service@localhost:3301) for replicaset(uuid="e0df68c5-e7f9-395f-86b3-30ad9e1b7b07", master=i1(pico_service@localhost:3301))
2024-02-08 11:55:11.450 [122699] main/130/lua/vshard.router I> All replicas are ok
2024-02-08 11:55:11.450 [122699] main/130/lua/vshard.router I> Failovering step is finished. Schedule next after 1.000000 seconds
2024-02-08 11:55:11.450 [122699] main/131/lua/vshard.util I> discovery_f has been started
2024-02-08 11:55:11.452 [122699] main/112/governor_loop I> updating current vshard config
2024-02-08 11:55:11.453 [122699] main/127/vshard.rebalancer/vshard.storage I> The cluster is balanced ok. Schedule next rebalancing after 3600.000000 seconds
2024-02-08 11:55:11.454 [122699] main/112/governor_loop I> updating sharding config, instance_id: i1
2024-02-08 11:55:11.455 [122699] main/112/governor_loop I> handling instance grade change, current_grade: Online(9), instance_id: i1
2024-02-08 11:55:11.460 [122699] main/131/vshard.discovery._static_router/vshard.router I> Updated replicaset(uuid="e0df68c5-e7f9-395f-86b3-30ad9e1b7b07", master=i1(pico_service@localhost:3301)) buckets: was 0, became 1000
2024-02-08 11:55:11.460 [122699] main/131/vshard.discovery._static_router/vshard.router I> Start aggressive discovery, 2000 buckets are unknown. Discovery works with 1 seconds interval
2024-02-08 11:55:11.557 [122699] main/112/governor_loop I> nothing to do, waiting for events to handle
2024-02-08 11:55:12.450 [122699] main/130/vshard.failover._static_router/vshard.router I> All replicas are ok
2024-02-08 11:55:12.472 [122699] main/131/vshard.discovery._static_router/vshard.router I> Updated replicaset(uuid="e0df68c5-e7f9-395f-86b3-30ad9e1b7b07", master=i1(pico_service@localhost:3301)) buckets: was 1000, became 2000
2024-02-08 11:55:13.484 [122699] main/131/vshard.discovery._static_router/vshard.router I> Updated replicaset(uuid="e0df68c5-e7f9-395f-86b3-30ad9e1b7b07", master=i1(pico_service@localhost:3301)) buckets: was 2000, became 3000
2024-02-08 11:55:13.485 [122699] main/131/vshard.discovery._static_router/vshard.router I> Discovery enters idle mode, all buckets are known. Discovery works with 10 seconds interval now
2024-02-08 11:55:20.332 [122699] main/113/sentinel_loop I> setting own target grade Offline
2024-02-08 11:55:20.333 [122699] main/129/localhost:3301 (net.box)/vshard.replicaset I> disconnected from localhost:3301
2024-02-08 11:55:20.333 [122699] main/129/localhost:3301 (net.box)/box.net_box net_box.lua:352 W> localhost:3301: Peer closed
2024-02-08 11:55:20.334 [122699] main/135/iproto.shutdown I> tx_binary: stopped
2024-02-08 11:55:20.334 [122699] main/128/localhost:3301 (net.box)/vshard.replicaset I> disconnected from localhost:3301
2024-02-08 11:55:20.334 [122699] main/128/localhost:3301 (net.box)/box.net_box net_box.lua:352 W> localhost:3301: Peer closed
2024-02-08 11:55:20.429 [122699] main/112/governor_loop plan.rs:80 W> leader is going offline and no substitution is found, voters: [1], leader_raft_id: 1
2024-02-08 11:55:20.429 [122699] main/112/governor_loop I> downgrading instance i1
2024-02-08 11:55:20.429 [122699] main/112/governor_loop I> handling instance grade change, current_grade: Offline(9), instance_id: i1
2024-02-08 11:55:20.532 [122699] main/134/trigger_fiber0 I> graceful shutdown succeeded
2024-02-08 11:55:20.532 [122699] main/112/governor_loop I> target master i1 of replicaset r1 is not online: trying to choose a new one
2024-02-08 11:55:20.532 [122699] main/112/governor_loop plan.rs:545 W> there are no instances suitable as master of replicaset r1
2024-02-08 11:55:20.532 [122699] main/112/governor_loop I> updating target vshard config
2024-02-08 11:55:20.533 [122699] main/112/governor_loop I> target master i1 of replicaset r1 is not online: trying to choose a new one
2024-02-08 11:55:20.533 [122699] main/112/governor_loop plan.rs:545 W> there are no instances suitable as master of replicaset r1
2024-02-08 11:55:20.533 [122699] main/112/governor_loop I> applying vshard config changes
2024-02-08 11:55:20.533 [122699] main/112/governor_loop I> updating current vshard config
2024-02-08 11:55:20.533 [122699] main/112/governor_loop I> target master i1 of replicaset r1 is not online: trying to choose a new one
2024-02-08 11:55:20.533 [122699] main/112/governor_loop plan.rs:545 W> there are no instances suitable as master of replicaset r1
2024-02-08 11:55:20.533 [122699] main/112/governor_loop I> nothing to do, waiting for events to handle
2024-02-08 11:55:29.267 [122735] main/103/interactive I> Tarantool 2.11.2-94-gcbd79b118 Linux-x86_64-RelWithDebInfo
2024-02-08 11:55:29.267 [122735] main/103/interactive I> log level 5
2024-02-08 11:55:29.268 [122735] main/103/interactive I> wal/engine cleanup is paused
2024-02-08 11:55:29.268 [122735] main/103/interactive I> mapping 67108864 bytes for memtx tuple arena...
2024-02-08 11:55:29.268 [122735] main/103/interactive I> Actual slab_alloc_factor calculated on the basis of desired slab_alloc_factor = 1.044274
2024-02-08 11:55:29.268 [122735] main/103/interactive I> mapping 134217728 bytes for vinyl tuple arena...
2024-02-08 11:55:29.268 [122735] main/103/interactive/box.upgrade I> Recovering snapshot with schema version 2.11.1
2024-02-08 11:55:29.270 [122735] main/103/interactive I> update replication_synchro_quorum = 1
2024-02-08 11:55:29.270 [122735] main/103/interactive I> instance uuid 68d4a766-4144-3248-aeb4-e212356716e4
2024-02-08 11:55:29.270 [122735] main/103/interactive I> instance vclock {0: 575, 1: 3137}
2024-02-08 11:55:29.270 [122735] main/103/interactive I> recovery start
2024-02-08 11:55:29.270 [122735] main/103/interactive I> recovering from `./00000000000000000000.snap'
2024-02-08 11:55:29.270 [122735] main/103/interactive I> cluster uuid e0df68c5-e7f9-395f-86b3-30ad9e1b7b07
2024-02-08 11:55:29.280 [122735] main/103/interactive I> assigned id 1 to replica 68d4a766-4144-3248-aeb4-e212356716e4
2024-02-08 11:55:29.280 [122735] main/103/interactive I> update replication_synchro_quorum = 1
2024-02-08 11:55:29.280 [122735] main/103/interactive I> recover from `./00000000000000000000.xlog'
2024-02-08 11:55:29.285 [122735] main/103/interactive I> done `./00000000000000000000.xlog'
2024-02-08 11:55:29.285 [122735] main/103/interactive I> recover from `./00000000000000003325.xlog'
2024-02-08 11:55:29.285 [122735] main/103/interactive I> done `./00000000000000003325.xlog'
2024-02-08 11:55:29.285 [122735] main/103/interactive I> recover from `./00000000000000003368.xlog'
2024-02-08 11:55:29.286 [122735] main/103/interactive I> done `./00000000000000003368.xlog'
2024-02-08 11:55:29.286 [122735] main/103/interactive I> recover from `./00000000000000003411.xlog'
2024-02-08 11:55:29.286 [122735] main/103/interactive I> done `./00000000000000003411.xlog'
2024-02-08 11:55:29.286 [122735] main/103/interactive I> recover from `./00000000000000003454.xlog'
2024-02-08 11:55:29.286 [122735] main/103/interactive I> done `./00000000000000003454.xlog'
2024-02-08 11:55:29.286 [122735] main/103/interactive I> recover from `./00000000000000003497.xlog'
2024-02-08 11:55:29.286 [122735] main/103/interactive I> done `./00000000000000003497.xlog'
2024-02-08 11:55:29.286 [122735] main/103/interactive I> recover from `./00000000000000003540.xlog'
2024-02-08 11:55:29.286 [122735] main/103/interactive I> done `./00000000000000003540.xlog'
2024-02-08 11:55:29.286 [122735] main/103/interactive I> recover from `./00000000000000003583.xlog'
2024-02-08 11:55:29.286 [122735] main/103/interactive I> done `./00000000000000003583.xlog'
2024-02-08 11:55:29.286 [122735] main/103/interactive I> recover from `./00000000000000003626.xlog'
2024-02-08 11:55:29.286 [122735] main/103/interactive I> done `./00000000000000003626.xlog'
2024-02-08 11:55:29.286 [122735] main/103/interactive I> recover from `./00000000000000003669.xlog'
2024-02-08 11:55:29.286 [122735] main/103/interactive I> done `./00000000000000003669.xlog'
2024-02-08 11:55:29.286 [122735] main/103/interactive I> recover from `./00000000000000003712.xlog'
2024-02-08 11:55:29.286 [122735] main/103/interactive I> done `./00000000000000003712.xlog'
2024-02-08 11:55:29.287 [122735] main/103/interactive I> Building secondary indexes in space '_pico_table'...
2024-02-08 11:55:29.287 [122735] main/103/interactive I> Adding 11 keys to TREE index 'name' ...
2024-02-08 11:55:29.287 [122735] main/103/interactive I> Space '_pico_table': done
2024-02-08 11:55:29.287 [122735] main/103/interactive I> Building secondary indexes in space '_pico_instance'...
2024-02-08 11:55:29.287 [122735] main/103/interactive I> Adding 1 keys to TREE index 'raft_id' ...
2024-02-08 11:55:29.287 [122735] main/103/interactive I> Adding 1 keys to TREE index 'replicaset_id' ...
2024-02-08 11:55:29.287 [122735] main/103/interactive I> Space '_pico_instance': done
2024-02-08 11:55:29.287 [122735] main/103/interactive I> Building secondary indexes in space '_pico_user'...
2024-02-08 11:55:29.287 [122735] main/103/interactive I> Adding 3 keys to TREE index 'name' ...
2024-02-08 11:55:29.287 [122735] main/103/interactive I> Space '_pico_user': done
2024-02-08 11:55:29.287 [122735] main/103/interactive I> Building secondary indexes in space '_pico_privilege'...
2024-02-08 11:55:29.287 [122735] main/103/interactive I> Adding 11 keys to TREE index 'object' ...
2024-02-08 11:55:29.287 [122735] main/103/interactive I> Space '_pico_privilege': done
2024-02-08 11:55:29.287 [122735] main/103/interactive I> Building secondary indexes in space '_bucket'...
2024-02-08 11:55:29.287 [122735] main/103/interactive I> Adding 3000 keys to TREE index 'status' ...
2024-02-08 11:55:29.289 [122735] main/103/interactive I> Space '_bucket': done
2024-02-08 11:55:29.289 [122735] main/103/interactive I> Building secondary indexes in space '_pico_role'...
2024-02-08 11:55:29.289 [122735] main/103/interactive I> Adding 2 keys to TREE index 'name' ...
2024-02-08 11:55:29.289 [122735] main/103/interactive I> Space '_pico_role': done
2024-02-08 11:55:29.289 [122735] main/103/interactive I> ready to accept requests
2024-02-08 11:55:29.289 [122735] main/103/interactive I> leaving orphan mode
2024-02-08 11:55:29.289 [122735] main/105/gc I> wal/engine cleanup is resumed
2024-02-08 11:55:29.289 [122735] main/103/interactive/box.load_cfg I> set 'memtx_memory' configuration option to 67108864
2024-02-08 11:55:29.289 [122735] main/103/interactive/box.load_cfg I> set 'log' configuration option to " "
2024-02-08 11:55:29.289 [122735] main/103/interactive/box.load_cfg I> set 'replication' configuration option to []
2024-02-08 11:55:29.289 [122735] main/103/interactive/box.load_cfg I> set 'replication_connect_quorum' configuration option to 32
2024-02-08 11:55:29.289 [122735] main/103/interactive/box.load_cfg I> set 'bootstrap_strategy' configuration option to "legacy"
2024-02-08 11:55:29.304 [122735] main/103/interactive/box.load_cfg load_cfg.lua:738 W> Deprecated option replication_connect_quorum, please use bootstrap_strategy instead
2024-02-08 11:55:29.304 [122735] main/106/checkpoint_daemon I> scheduled next checkpoint for Thu Feb  8 13:50:46 2024
2024-02-08 11:55:29.304 [122735] main/103/interactive I> tx_binary: bound to 127.0.0.1:3301
2024-02-08 11:55:29.304 [122735] main/103/interactive/box.load_cfg I> set 'listen' configuration option to "localhost:3301"
2024-02-08 11:55:29.304 [122735] main/103/interactive/box.load_cfg I> set 'read_only' configuration option to true
2024-02-08 11:55:29.305 [122735] main/103/interactive I> >>>>> postjoin()
2024-02-08 11:55:29.305 [122735] main/103/interactive/box.load_cfg load_cfg.lua:738 W> Deprecated option replication_connect_quorum, please use bootstrap_strategy instead
2024-02-08 11:55:29.305 [122735] main/103/interactive I> leaving orphan mode
2024-02-08 11:55:29.305 [122735] main/103/interactive/box.load_cfg I> set 'replication_connect_quorum' configuration option to 0
2024-02-08 11:55:29.306 [122735] main/103/interactive I> switched to configuration, config: Configuration { voters: Configuration { incoming: Configuration { voters: {1} }, outgoing: Configuration { voters: {} } }, learners: {}, learners_next: {}, auto_leave: false }, raft_id: 1
2024-02-08 11:55:29.306 [122735] main/103/interactive I> became follower at term 11, term: 11, raft_id: 1
2024-02-08 11:55:29.306 [122735] main/103/interactive I> newRaft, peers: Configuration { incoming: Configuration { voters: {1} }, outgoing: Configuration { voters: {} } }, last term: 11, last index: 142, applied: 142, commit: 142, term: 11, raft_id: 1
2024-02-08 11:55:29.306 [122735] main/103/interactive I> RawNode created with id 1., id: 1, raft_id: 1
2024-02-08 11:55:29.306 [122735] main/113/sentinel_loop I> waiting until initialized...
2024-02-08 11:55:29.306 [122735] main/103/interactive I> this is the only voter in cluster, triggering election immediately
2024-02-08 11:55:29.306 [122735] main/103/interactive I> starting a new election, term: 11, raft_id: 1
2024-02-08 11:55:29.307 [122735] main/103/interactive I> became pre-candidate at term 11, term: 11, raft_id: 1
2024-02-08 11:55:29.307 [122735] main/103/interactive I> became candidate at term 12, term: 12, raft_id: 1
2024-02-08 11:55:29.307 [122735] main/103/interactive I> became leader at term 12, term: 12, raft_id: 1
2024-02-08 11:55:29.307 [122735] main/103/interactive/box.load_cfg load_cfg.lua:738 W> Deprecated option replication_connect_quorum, please use bootstrap_strategy instead
2024-02-08 11:55:29.307 [122735] main/103/interactive/socket I> tcp_server: remove dead UNIX socket: ./admin.sock
2024-02-08 11:55:29.308 [122735] main/112/governor_loop I> target master i1 of replicaset r1 is not online: trying to choose a new one
2024-02-08 11:55:29.308 [122735] main/112/governor_loop plan.rs:545 W> there are no instances suitable as master of replicaset r1
2024-02-08 11:55:29.308 [122735] main/112/governor_loop I> nothing to do, waiting for events to handle
2024-02-08 11:55:29.308 [122735] main/114/console/unix/:./admin.sock/socket I> started
2024-02-08 11:55:29.308 [122735] main/103/interactive I> initiating self-activation of i1
2024-02-08 11:55:29.411 [122735] main/112/governor_loop I> configuring replication
2024-02-08 11:55:29.411 [122735] main/112/governor_loop I> calling rpc::replication, instance_id: i1
2024-02-08 11:55:29.412 [122735] main/103/interactive I> self-activated successfully
2024-02-08 11:55:29.412 [122735] main I> entering the event loop
2024-02-08 11:55:29.412 [122735] main/110/.proc_replication I> connecting to 1 replicas
2024-02-08 11:55:29.412 [122735] main/110/.proc_replication C> failed to connect to 1 out of 1 replicas
2024-02-08 11:55:29.413 [122735] main/110/.proc_replication I> leaving orphan mode
2024-02-08 11:55:29.413 [122735] main/110/.proc_replication/box.load_cfg I> set 'replication' configuration option to ["pico_service@localhost:3301"]
2024-02-08 11:55:29.413 [122735] main/110/.proc_replication/box.load_cfg I> set 'read_only' configuration option to false
2024-02-08 11:55:29.413 [122735] main/112/governor_loop I> configured replication with instance, lsn: 3137, instance_id: i1
2024-02-08 11:55:29.413 [122735] main/112/governor_loop I> handling instance grade change, current_grade: Replicated(11), instance_id: i1
2024-02-08 11:55:29.413 [122735] main/121/applier/pico_service@localhost:3301 I> remote master 68d4a766-4144-3248-aeb4-e212356716e4 at 127.0.0.1:3301 running Tarantool 2.11.2
2024-02-08 11:55:29.414 [122735] main/121/applier/pico_service@localhost:3301 I> leaving orphan mode
2024-02-08 11:55:29.514 [122735] main/112/governor_loop I> updating target vshard config
2024-02-08 11:55:29.514 [122735] main/112/governor_loop I> applying vshard config changes
2024-02-08 11:55:29.514 [122735] main/112/governor_loop I> calling rpc::sharding, instance_id: i1
2024-02-08 11:55:29.515 [122735] main/110/.proc_sharding/vshard.storage I> Starting configuration of replica 68d4a766-4144-3248-aeb4-e212356716e4
2024-02-08 11:55:29.515 [122735] main/110/.proc_sharding/vshard.storage I> I am master
2024-02-08 11:55:29.515 [122735] main/110/.proc_sharding/vshard.storage I> Taking on replicaset master role...
2024-02-08 11:55:29.515 [122735] main/110/.proc_sharding I> tx_binary: stopped
2024-02-08 11:55:29.515 [122735] main/110/.proc_sharding I> tx_binary: bound to 127.0.0.1:3301
2024-02-08 11:55:29.515 [122735] main/110/.proc_sharding/box.load_cfg I> set 'listen' configuration option to "127.0.0.1:3301"
2024-02-08 11:55:29.516 [122735] main/110/.proc_sharding/vshard.storage I> Box has been configured
2024-02-08 11:55:29.516 [122735] main/125/lua/vshard.util I> gc_bucket_f has been started
2024-02-08 11:55:29.516 [122735] main/125/lua/box.schema schema.lua:1832 W> box.internal.schema_version will be removed, please use box.info.schema_version instead
2024-02-08 11:55:29.516 [122735] main/126/lua/vshard.util I> recovery_f has been started
2024-02-08 11:55:29.516 [122735] main/110/.proc_sharding/vshard.storage I> Took on replicaset master role
2024-02-08 11:55:29.517 [122735] main/127/lua/vshard.util I> rebalancer_f has been started
2024-02-08 11:55:29.519 [122735] main/110/.proc_sharding/vshard.router I> Starting router configuration
2024-02-08 11:55:29.519 [122735] main/110/.proc_sharding/vshard.router I> Calling box.cfg()...
2024-02-08 11:55:29.519 [122735] main/110/.proc_sharding/vshard.router I> {"listen":"127.0.0.1:3301"}
2024-02-08 11:55:29.519 [122735] main/110/.proc_sharding/vshard.router I> Box has been configured
2024-02-08 11:55:29.520 [122735] main/128/localhost:3301 (net.box)/vshard.replicaset I> connected to localhost:3301
2024-02-08 11:55:29.521 [122735] main/129/localhost:3301 (net.box)/vshard.replicaset I> connected to localhost:3301
2024-02-08 11:55:29.521 [122735] main/130/lua/vshard.util I> failover_f has been started
2024-02-08 11:55:29.521 [122735] main/130/lua/vshard.router I> New replica i1(pico_service@localhost:3301) for replicaset(uuid="e0df68c5-e7f9-395f-86b3-30ad9e1b7b07", master=i1(pico_service@localhost:3301))
2024-02-08 11:55:29.521 [122735] main/130/lua/vshard.router I> All replicas are ok
2024-02-08 11:55:29.521 [122735] main/130/lua/vshard.router I> Failovering step is finished. Schedule next after 1.000000 seconds
2024-02-08 11:55:29.521 [122735] main/131/lua/vshard.util I> discovery_f has been started
2024-02-08 11:55:29.523 [122735] main/112/governor_loop I> updating current vshard config
2024-02-08 11:55:29.523 [122735] main/127/vshard.rebalancer/vshard.storage I> The cluster is balanced ok. Schedule next rebalancing after 3600.000000 seconds
2024-02-08 11:55:29.524 [122735] main/112/governor_loop I> updating sharding config, instance_id: i1
2024-02-08 11:55:29.524 [122735] main/112/governor_loop I> handling instance grade change, current_grade: Online(11), instance_id: i1
2024-02-08 11:55:29.531 [122735] main/131/vshard.discovery._static_router/vshard.router I> Updated replicaset(uuid="e0df68c5-e7f9-395f-86b3-30ad9e1b7b07", master=i1(pico_service@localhost:3301)) buckets: was 0, became 1000
2024-02-08 11:55:29.531 [122735] main/131/vshard.discovery._static_router/vshard.router I> Start aggressive discovery, 2000 buckets are unknown. Discovery works with 1 seconds interval
2024-02-08 11:55:29.626 [122735] main/112/governor_loop I> nothing to do, waiting for events to handle
2024-02-08 11:55:30.522 [122735] main/130/vshard.failover._static_router/vshard.router I> All replicas are ok
2024-02-08 11:55:30.543 [122735] main/131/vshard.discovery._static_router/vshard.router I> Updated replicaset(uuid="e0df68c5-e7f9-395f-86b3-30ad9e1b7b07", master=i1(pico_service@localhost:3301)) buckets: was 1000, became 2000
2024-02-08 11:55:31.555 [122735] main/131/vshard.discovery._static_router/vshard.router I> Updated replicaset(uuid="e0df68c5-e7f9-395f-86b3-30ad9e1b7b07", master=i1(pico_service@localhost:3301)) buckets: was 2000, became 3000
2024-02-08 11:55:31.555 [122735] main/131/vshard.discovery._static_router/vshard.router I> Discovery enters idle mode, all buckets are known. Discovery works with 10 seconds interval now
2024-02-08 11:58:44.026 [122735] main/113/sentinel_loop I> setting own target grade Offline
2024-02-08 11:58:44.027 [122735] main/128/localhost:3301 (net.box)/vshard.replicaset I> disconnected from localhost:3301
2024-02-08 11:58:44.027 [122735] main/128/localhost:3301 (net.box)/box.net_box net_box.lua:352 W> localhost:3301: Peer closed
2024-02-08 11:58:44.027 [122735] main/129/localhost:3301 (net.box)/vshard.replicaset I> disconnected from localhost:3301
2024-02-08 11:58:44.027 [122735] main/129/localhost:3301 (net.box)/box.net_box net_box.lua:352 W> localhost:3301: Peer closed
2024-02-08 11:58:44.027 [122735] main/135/iproto.shutdown I> tx_binary: stopped
2024-02-08 11:58:44.077 [122735] main/112/governor_loop plan.rs:80 W> leader is going offline and no substitution is found, voters: [1], leader_raft_id: 1
2024-02-08 11:58:44.077 [122735] main/112/governor_loop I> downgrading instance i1
2024-02-08 11:58:44.077 [122735] main/112/governor_loop I> handling instance grade change, current_grade: Offline(11), instance_id: i1
2024-02-08 11:58:44.179 [122735] main/134/trigger_fiber0 I> graceful shutdown succeeded
2024-02-08 11:58:44.179 [122735] main/112/governor_loop I> target master i1 of replicaset r1 is not online: trying to choose a new one
2024-02-08 11:58:44.179 [122735] main/112/governor_loop plan.rs:545 W> there are no instances suitable as master of replicaset r1
2024-02-08 11:58:44.179 [122735] main/112/governor_loop I> updating target vshard config
2024-02-08 11:58:44.180 [122735] main/112/governor_loop I> target master i1 of replicaset r1 is not online: trying to choose a new one
2024-02-08 11:58:44.180 [122735] main/112/governor_loop plan.rs:545 W> there are no instances suitable as master of replicaset r1
2024-02-08 11:58:44.180 [122735] main/112/governor_loop I> applying vshard config changes
2024-02-08 11:58:44.180 [122735] main/112/governor_loop I> updating current vshard config
2024-02-08 11:58:44.180 [122735] main/112/governor_loop I> target master i1 of replicaset r1 is not online: trying to choose a new one
2024-02-08 11:58:44.180 [122735] main/112/governor_loop plan.rs:545 W> there are no instances suitable as master of replicaset r1
2024-02-08 11:58:44.180 [122735] main/112/governor_loop I> nothing to do, waiting for events to handle
